# 适配本地模型，例如vllm, ollama, xinference等
## Step1: 按需修改下面的create_model.py文件
```
文件: backend/slide_agent/slide_agent/create_model.py
文件: backend/simpleOutline/create_model.py
文件: backend/simplePPT/create_model.py
文件: backend/hostAgentAPI/hosts/multiagent/create_model.py
文件: backend/super_agent/create_model.py
文件: backend/super_agent/simpleOutline/create_model.py
文件: backend/super_agent/simpleArtical/create_model.py
```

运行某个vllm模型： vllm serve Qwen/Qwen2.5-1.5B-Instruct，注意得到api_base地址，然后修改代码.
例如添加vllm模型:
```
    elif provider == "vllm":
        # vllm模型配置
        assert os.environ.get("VLLM_API_KEY"), "VLLM_API_KEY is not set"
        if not model.startswith("openai/"):
            # 表示兼容openai的模型请求
            model = "openai/" + model
        return LiteLlm(model=model, api_key=os.environ.get("VLLM_API_KEY"), api_base="http://localhost:8000/v1")
```
##  Step2:  按需在.env或者main.py中指定不同的provider和模型即可